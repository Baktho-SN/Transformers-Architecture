{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Self attention"
      ],
      "metadata": {
        "id": "e77jT3-f5n-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So lets try to recreate self attention, cuz why not.\n",
        "\n",
        "---\n",
        "\n",
        "First lets take a sentence, a beginner level one at that \"My name is Baktho\" <br>\n",
        "Here let L be the length of the input = 4\n",
        "<br>\n",
        "Let d_k and d_v be the length of the arrays q,k and v\n",
        "<br>\n",
        ">Every single input for a transformer will have these 3 vectors:\n",
        ">* q - a query vector (\"What I'm looking for\")\n",
        "* k - a key vector (\"What I can offer/ What I have\")\n",
        "* v - a Value vector (\"What I actually\n",
        " offer\")"
      ],
      "metadata": {
        "id": "4jzVZQ_Y5uo1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "3yqOSCg8oAJ7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "L,d_k,d_v = 4,8,8\n",
        "q = np.random.randn(L,d_k)\n",
        "k = np.random.randn(L,d_k)\n",
        "v = np.random.randn(L,d_v)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q\\n\",q,\"\\nK\\n\",k,\"\\nV\\n\",v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJMIc6p3oX_e",
        "outputId": "3ca84e76-3513-4439-baf1-af2baae6df71"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q\n",
            " [[-0.91650365 -0.43938416 -0.98441261 -0.65332904  0.268756   -0.29742282\n",
            "   0.0374929  -1.89440144]\n",
            " [-0.06346317 -0.1609114   1.73088298  0.30103895  0.89140338  0.75865651\n",
            "   0.41346719  0.95797565]\n",
            " [-0.2178364  -1.48619427  0.02730158 -1.16561411 -0.17503116 -0.65109247\n",
            "   1.42877914  2.02763922]\n",
            " [-0.76685037  0.67228886  0.55178884 -0.77246581  0.45459953  1.11097801\n",
            "  -1.19093102 -0.96136145]] \n",
            "K\n",
            " [[ 0.25498692 -1.1890085  -1.47267718  1.42860365 -1.36587921  0.57772352\n",
            "  -0.52634499 -1.24090347]\n",
            " [-1.66286252  0.65208971 -0.4453618  -0.30204426  0.42928219  1.2096245\n",
            "   2.33289997 -0.57796015]\n",
            " [-1.67234527  0.099901   -1.83971522  0.14641315 -0.13421936 -1.01376185\n",
            "  -0.49231473  1.38570551]\n",
            " [ 1.48226781 -1.10782742  1.45953526 -0.74570896  0.40941078 -0.16764686\n",
            "  -0.62440186 -0.16653004]] \n",
            "V\n",
            " [[ 0.42672627  0.67092111 -0.45006872 -1.00261084  1.11385623  0.48990995\n",
            "   1.0427403   0.2461847 ]\n",
            " [ 1.81759508  2.83380172 -1.50073764  0.1541898  -0.3610862   0.94638871\n",
            "   0.69839095 -0.06249757]\n",
            " [ 1.74007731 -1.58161123  0.26265871  0.08798802  0.46236134  1.05376072\n",
            "   0.63312742 -0.06617454]\n",
            " [ 0.35752199  0.34235483 -0.68491185 -2.1364204   0.75448204  0.95303359\n",
            "  -0.88082517  0.21849094]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self attention\n",
        "\n",
        "self attention = softmax((Q.Kᵀ ÷ √dk) + M)V\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lm20O7NzpDZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mm = np.matmul(q,k.T)\n",
        "mm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFpgjnIvpDGL",
        "outputId": "13295d3a-76d2-4e1e-99f9-4e0c3caccb82"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.59722752,  2.81121371,  0.82610135, -1.36937592],\n",
              "       [-4.12946109,  0.85006455, -2.81502306,  2.20605231],\n",
              "       [-3.39907008,  1.03159696,  2.78478409,  1.04031016],\n",
              "       [-1.0703409 ,  1.01745595, -1.71176672,  0.40351338]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Why we need sqrt(d_k) in denominator? To minimize variance and stabilize the values of Q.K^T\n",
        "q.var(), k.var(), mm.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO94DvK2ojXM",
        "outputId": "eb13e99a-611f-4f92-da85-560c9d55c7db"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8643864049699872, 1.1253028897510355, 4.6518239646636275)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above output, the variance of the matrix multiplication is much higher"
      ],
      "metadata": {
        "id": "TXkQIb8zuXug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaled = mm/math.sqrt(d_k)\n",
        "q.var(), k.var(), scaled.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81txY10Et8R8",
        "outputId": "08baf025-302e-4f96-c532-8bafd49f023d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8643864049699872, 1.1253028897510355, 0.5814779955829534)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the variances are more or less in the same range"
      ],
      "metadata": {
        "id": "RoKikeRNuf3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaKFs3cOuQ9Y",
        "outputId": "333f117a-349a-4290-8ae4-4041abfac5ed"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.91825859,  0.99391414,  0.29207093, -0.4841475 ],\n",
              "       [-1.45998497,  0.3005432 , -0.99526095,  0.77995727],\n",
              "       [-1.20175275,  0.3647246 ,  0.98456986,  0.36780519],\n",
              "       [-0.37842265,  0.359725  , -0.60520093,  0.14266352]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above output even the actual values are now in the same range"
      ],
      "metadata": {
        "id": "eF752mNOuvvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masking\n",
        "*   This is to ensure words don't get context from words generated in the future.\n",
        "*   Not required in the encoders but in the decoders\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "KJJYwZSFvUGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.tril(np.ones( (L,L) )) # triangular matrix\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veaLurLIutlN",
        "outputId": "433200d8-2c3b-4f04-96f1-adc99876740b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 1., 0.],\n",
              "       [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the first element of first row can only look at itself, the elements in the second row can only look at itself and one before it(cuz rest are 0-> Masked) and so on"
      ],
      "metadata": {
        "id": "kZme2ZSawsnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask[mask==0] = -np.Infinity\n",
        "mask[mask==1] = 0"
      ],
      "metadata": {
        "id": "zAlY1lY_wUCV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cl3qTiCjwde4",
        "outputId": "08e59497-554a-4e6e-9363-2c05065a8b14"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., -inf],\n",
              "       [  0.,   0.,   0.,   0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked = scaled+mask\n",
        "masked"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzUh7hnBwe9b",
        "outputId": "574ba05b-5d70-49a6-eea8-be85825df44d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.91825859,        -inf,        -inf,        -inf],\n",
              "       [-1.45998497,  0.3005432 ,        -inf,        -inf],\n",
              "       [-1.20175275,  0.3647246 ,  0.98456986,        -inf],\n",
              "       [-0.37842265,  0.359725  , -0.60520093,  0.14266352]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "why use zero and -infi? -> for the above addition and for the softmax operation"
      ],
      "metadata": {
        "id": "xKtqGr-QxRiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Softmax\n",
        "\n",
        "SOFTMAX = eˣⁱ / ∑<sub>j</sub>eˣ<sub>j</sub>\n",
        "\n",
        "> Converts vectors into probability distribution so the vectors add upto one"
      ],
      "metadata": {
        "id": "3fGIIfe6xf7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T/np.sum(np.exp(x), axis=-1)).T"
      ],
      "metadata": {
        "id": "BUz-embbwhto"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention = softmax(masked)"
      ],
      "metadata": {
        "id": "JDOTqx5pzV1k"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry5_cObYzexf",
        "outputId": "63bc54da-d9a5-41d8-b39e-52fe98f873d0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.        , 0.        , 0.        ],\n",
              "       [0.1467242 , 0.8532758 , 0.        , 0.        ],\n",
              "       [0.06806351, 0.3260069 , 0.60592959, 0.        ],\n",
              "       [0.17943625, 0.37539082, 0.14302819, 0.30214474]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_v = np.matmul(attention,v)\n",
        "new_v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6UYOeAYzee4",
        "outputId": "c5becc13-be69-49a8-a533-f1c20e277c44"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.42672627,  0.67092111, -0.45006872, -1.00261084,  1.11385623,\n",
              "         0.48990995,  1.0427403 ,  0.2461847 ],\n",
              "       [ 1.61352097,  2.51645479, -1.34657908, -0.01554085, -0.14467645,\n",
              "         0.87941223,  0.74891533, -0.01720641],\n",
              "       [ 1.67595736,  0.0111591 , -0.3607314 ,  0.03534027,  0.23825479,\n",
              "         0.98037904,  0.68228347, -0.04371556],\n",
              "       [ 1.11578216,  1.06139643, -0.81349669, -0.75494672,  0.35841123,\n",
              "         0.88184481,  0.27369333,  0.07726451]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptIPMP4514jT",
        "outputId": "f022aa08-a792-46aa-93b7-bbfe61330f62"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.42672627,  0.67092111, -0.45006872, -1.00261084,  1.11385623,\n",
              "         0.48990995,  1.0427403 ,  0.2461847 ],\n",
              "       [ 1.81759508,  2.83380172, -1.50073764,  0.1541898 , -0.3610862 ,\n",
              "         0.94638871,  0.69839095, -0.06249757],\n",
              "       [ 1.74007731, -1.58161123,  0.26265871,  0.08798802,  0.46236134,\n",
              "         1.05376072,  0.63312742, -0.06617454],\n",
              "       [ 0.35752199,  0.34235483, -0.68491185, -2.1364204 ,  0.75448204,\n",
              "         0.95303359, -0.88082517,  0.21849094]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first row of the Value and new Value is pretty much same but the rest are way different because of the self attention mechanism"
      ],
      "metadata": {
        "id": "Ele5TdPf2LRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summing it up"
      ],
      "metadata": {
        "id": "v3IZ2Y3Q2Ho3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T/np.sum(np.exp(x), axis=-1)).T\n",
        "\n",
        "def scaled_dot_product_attention(q,k,v, mask = None):\n",
        "  d_k = q.shape[-1]\n",
        "  mm = np.matmul(q,k.T)\n",
        "  scaled = mm/math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scaled = scaled+mask\n",
        "  attention = softmax(scaled)\n",
        "  output = np.matmul(attention,v)\n",
        "  return output, attention"
      ],
      "metadata": {
        "id": "9iNap1jW16H2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values, attention = scaled_dot_product_attention(q,k,v, mask=mask)\n",
        "print(\"Q\\n\",q,\"\\nK\\n\",k,\"\\nV\\n\",v)\n",
        "print(\"Values\\n\",values,\"\\nAttention\\n\",attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgmQsxhK3Qa-",
        "outputId": "b67500fa-b6f1-486b-976d-b57cfc3cac2e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q\n",
            " [[-0.91650365 -0.43938416 -0.98441261 -0.65332904  0.268756   -0.29742282\n",
            "   0.0374929  -1.89440144]\n",
            " [-0.06346317 -0.1609114   1.73088298  0.30103895  0.89140338  0.75865651\n",
            "   0.41346719  0.95797565]\n",
            " [-0.2178364  -1.48619427  0.02730158 -1.16561411 -0.17503116 -0.65109247\n",
            "   1.42877914  2.02763922]\n",
            " [-0.76685037  0.67228886  0.55178884 -0.77246581  0.45459953  1.11097801\n",
            "  -1.19093102 -0.96136145]] \n",
            "K\n",
            " [[ 0.25498692 -1.1890085  -1.47267718  1.42860365 -1.36587921  0.57772352\n",
            "  -0.52634499 -1.24090347]\n",
            " [-1.66286252  0.65208971 -0.4453618  -0.30204426  0.42928219  1.2096245\n",
            "   2.33289997 -0.57796015]\n",
            " [-1.67234527  0.099901   -1.83971522  0.14641315 -0.13421936 -1.01376185\n",
            "  -0.49231473  1.38570551]\n",
            " [ 1.48226781 -1.10782742  1.45953526 -0.74570896  0.40941078 -0.16764686\n",
            "  -0.62440186 -0.16653004]] \n",
            "V\n",
            " [[ 0.42672627  0.67092111 -0.45006872 -1.00261084  1.11385623  0.48990995\n",
            "   1.0427403   0.2461847 ]\n",
            " [ 1.81759508  2.83380172 -1.50073764  0.1541898  -0.3610862   0.94638871\n",
            "   0.69839095 -0.06249757]\n",
            " [ 1.74007731 -1.58161123  0.26265871  0.08798802  0.46236134  1.05376072\n",
            "   0.63312742 -0.06617454]\n",
            " [ 0.35752199  0.34235483 -0.68491185 -2.1364204   0.75448204  0.95303359\n",
            "  -0.88082517  0.21849094]]\n",
            "Values\n",
            " [[ 0.42672627  0.67092111 -0.45006872 -1.00261084  1.11385623  0.48990995\n",
            "   1.0427403   0.2461847 ]\n",
            " [ 1.61352097  2.51645479 -1.34657908 -0.01554085 -0.14467645  0.87941223\n",
            "   0.74891533 -0.01720641]\n",
            " [ 1.67595736  0.0111591  -0.3607314   0.03534027  0.23825479  0.98037904\n",
            "   0.68228347 -0.04371556]\n",
            " [ 1.11578216  1.06139643 -0.81349669 -0.75494672  0.35841123  0.88184481\n",
            "   0.27369333  0.07726451]] \n",
            "Attention\n",
            " [[1.         0.         0.         0.        ]\n",
            " [0.1467242  0.8532758  0.         0.        ]\n",
            " [0.06806351 0.3260069  0.60592959 0.        ]\n",
            " [0.17943625 0.37539082 0.14302819 0.30214474]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a single simple attention head and transformers have multiple of the results stacked on top of each other in order to get multi head attention"
      ],
      "metadata": {
        "id": "syLtodCI5Ade"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xtvhb4Gt3z1d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}